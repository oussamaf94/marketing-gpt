{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u9nv3RY3gNTc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Generate segment data\n",
        "num_samples = 3000\n",
        "segments_df = pd.DataFrame({\n",
        "    'Age': [random.randint(18, 70) for _ in range(num_samples)],\n",
        "    'Income': [random.choice(['low', 'medium', 'high']) for _ in range(num_samples)],\n",
        "    'Gender': [random.choice(['male', 'female']) for _ in range(num_samples)],\n",
        "    'Behavior': [random.choice(['tech-savvy', 'traditional', 'budget-conscious'])\n",
        "                for _ in range(num_samples)]\n",
        "})\n",
        "\n",
        "# Generate corresponding messages\n",
        "def generate_message(row):\n",
        "    if row['Behavior'] == 'tech-savvy':\n",
        "        return f\"Discover the latest technology tailored for {row['Gender']} {row['Age']} years old!\"\n",
        "    elif row['Behavior'] == 'traditional':\n",
        "        return f\"Check out our classic products for {row['Gender']} aged {row['Age']}.\"\n",
        "    else:\n",
        "        return f\"Budget-friendly options for {row['Gender']} at {row['Age']} years!\"\n",
        "\n",
        "# Create final dataset\n",
        "segments_df['Message'] = segments_df.apply(generate_message, axis=1)\n",
        "final_dataset = segments_df[['Age', 'Income', 'Gender', 'Behavior', 'Message']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract segments and messages\n",
        "segment_columns = ['Age', 'Income', 'Gender', 'Behavior']\n",
        "message_column = 'Message'\n",
        "\n",
        "# Create segment-message pairs\n",
        "segment_message_pairs = []\n",
        "for index, row in final_dataset.iterrows():\n",
        "    segment = tuple(row[segment_columns])\n",
        "    message = row[message_column]\n",
        "    segment_message_pairs.append((segment, message))"
      ],
      "metadata": {
        "id": "OHDl4bN-gU92"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary (alphabet, numbers, spaces, and punctuation)\n",
        "# Define character set and vocabulary\n",
        "chars = sorted(set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 ,.!-()\\''))\n",
        "vocab_size = len(chars) + 1  # Add 1 for padding token\n",
        "stoi = {ch:i+1 for i,ch in enumerate(chars)}  # Shift all indices by 1\n",
        "itos = {i+1:ch for i,ch in enumerate(chars)}  # Shift all indices by 1\n",
        "#itos[0] = ''  # Padding token\n",
        "\n",
        "# Extract segments and messages separately from the pairs\n",
        "segments = [str(pair[0]) for pair in segment_message_pairs]  # Get segments\n",
        "messages = [pair[1] for pair in segment_message_pairs]  # Get messages\n",
        "\n",
        "# Calculate maximum lengths\n",
        "seg_max = max(len(segment) for segment in segments)\n",
        "message_max = max(len(message) for message in messages)\n",
        "\n",
        "def encode(sentence, length=seg_max):\n",
        "    indicies = []\n",
        "    for c in sentence:\n",
        "        indicies.append(stoi[c])\n",
        "\n",
        "    # Add padding\n",
        "\n",
        "    if length is not None and len(indicies) < length:\n",
        "        indicies += [0] * (length - len(indicies))\n",
        "\n",
        "    return indicies\n",
        "\n",
        "def decode(sequence):\n",
        "    sentence = []\n",
        "    for i in sequence:\n",
        "        if i != 0:  # Skip padding tokens\n",
        "            sentence.append(itos[i])\n",
        "    return ''.join(sentence)\n",
        "\n",
        "\n",
        "\n",
        "encoded_pairs = []\n",
        "for segment, message in segment_message_pairs:\n",
        "    # Convert segment tuple to string\n",
        "    segment_str = str(segment)  # Convert tuple to string\n",
        "\n",
        "    # Encode both segment and message\n",
        "    segment_encoded = encode(segment_str, seg_max)\n",
        "    message_encoded = encode(message, message_max)\n",
        "\n",
        "    encoded_pairs.append((segment_encoded, message_encoded))"
      ],
      "metadata": {
        "id": "j_ZyyJ3qgY_f"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config[\"n_embed\"] % config[\"n_heads\"] == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config[\"n_embed\"], 3 * config[\"n_embed\"])\n",
        "        self.c_proj = nn.Linear(config[\"n_embed\"], config[\"n_embed\"])\n",
        "        self.attn_dropout = nn.Dropout(config[\"dropout\"])\n",
        "        self.resid_dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        # Assign config values to instance variables\n",
        "        self.n_heads = config[\"n_heads\"]\n",
        "        self.n_embed = config[\"n_embed\"]\n",
        "        self.seg_length = config[\"seg_length\"]\n",
        "        self.message_length = config[\"message_length\"]\n",
        "        self.block_size = config[\"block_size\"]\n",
        "\n",
        "        # Fixed causal mask for training\n",
        "        total_length = self.seg_length + self.message_length\n",
        "        mask = torch.zeros(total_length, total_length)\n",
        "\n",
        "        # Full 1s for the segment portion\n",
        "        mask[:self.seg_length, :self.seg_length] = 1\n",
        "\n",
        "        # Lower triangular 1s for the message portion\n",
        "        for i in range(self.seg_length, total_length):\n",
        "            mask[i, :i + 1] = 1\n",
        "\n",
        "        # Reshape to 4D for attention (1, 1, total_length, total_length)\n",
        "        mask = mask.view(1, 1, total_length, total_length)\n",
        "\n",
        "        # Register the mask as a buffer\n",
        "        self.register_buffer(\"fixed_causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        # Linear projections for query, key, value\n",
        "        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
        "\n",
        "        # Reshape projections for multi-head attention\n",
        "        k = k.view(B, T, self.n_heads, self.n_embed // self.n_heads).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_heads, self.n_embed // self.n_heads).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_heads, self.n_embed // self.n_heads).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # Compute attention scores\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        # Apply the fixed causal mask during training or slice for generation\n",
        "        mask = self.fixed_causal_mask[:, :, :T, :T]  # Dynamically slice the mask for the current sequence length\n",
        "        att = att.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Compute attention probabilities and apply dropout\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        y = att @ v\n",
        "\n",
        "        # Reshape and project back to original embedding size\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, self.n_embed)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "7ZwUvbmmiQeY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config[\"n_embed\"], 4*config[\"n_embed\"])\n",
        "    self.c_proj=nn.Linear(4*config[\"n_embed\"], config[\"n_embed\"])\n",
        "    self.drop= nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.c_fc(x)\n",
        "    x = F.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    x = self.drop(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ooNysVFhlFFx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_1 = nn.LayerNorm(config[\"n_embed\"])\n",
        "        self.ln_2 = nn.LayerNorm(config[\"n_embed\"])\n",
        "        self.ff = MLP(config)  # Using the updated MLP class name\n",
        "\n",
        "        # Optional: Initialize layer norm parameters\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize layer norm weights and biases\n",
        "        self.ln_1.bias.data.zero_()\n",
        "        self.ln_1.weight.data.fill_(1.0)\n",
        "        self.ln_2.bias.data.zero_()\n",
        "        self.ln_2.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Residual connections maintain device of input tensor\n",
        "        x = x + self.attn(self.ln_1(x))  # First residual connection\n",
        "        x = x + self.ff(self.ln_2(x))    # Second residual connection\n",
        "        return x"
      ],
      "metadata": {
        "id": "CrKpd4wWmA2-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config[\"vocab_size\"] is not None\n",
        "        assert config[\"block_size\"] is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            'wte': nn.Embedding(config[\"vocab_size\"], config[\"n_embed\"]),\n",
        "            'wpe': nn.Embedding(config[\"block_size\"], config[\"n_embed\"]),\n",
        "            'drop': nn.Dropout(config[\"dropout\"]),\n",
        "            'h': nn.ModuleList([Block(config) for _ in range(config[\"n_layers\"])]),\n",
        "            'ln_f': nn.LayerNorm(config[\"n_embed\"])\n",
        "        })\n",
        "\n",
        "        self.lm_head = nn.Linear(config[\"n_embed\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "        # Tie weights between embedding and output layer\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize embedding layers\n",
        "        nn.init.normal_(self.transformer.wte.weight, std=0.02)\n",
        "        nn.init.normal_(self.transformer.wpe.weight, std=0.02)\n",
        "\n",
        "        # Initialize layer norm\n",
        "        nn.init.normal_(self.transformer.ln_f.weight, std=0.02)\n",
        "        nn.init.zeros_(self.transformer.ln_f.bias)\n",
        "\n",
        "    def forward(self, segment, message=None):\n",
        "        device = segment.device\n",
        "\n",
        "        # Combine segment and message\n",
        "        if message is not None:\n",
        "\n",
        "            idx = torch.cat([segment, message], dim=1)\n",
        "\n",
        "        else:\n",
        "\n",
        "            idx = segment\n",
        "\n",
        "\n",
        "        B, T = idx.size()\n",
        "        # Block size check\n",
        "        assert T <= self.config[\"block_size\"], f\"Cannot forward sequence of length {T}, block size is {self.config['block_size']}\"\n",
        "\n",
        "        # Create position tensor directly on correct device\n",
        "        pos = torch.arange(0, T, device=device).unsqueeze(0)\n",
        "\n",
        "        # Embeddings\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = None\n",
        "        if message is not None:\n",
        "            # Get message logits and compute loss\n",
        "            message_start = segment.size(1)\n",
        "            logits_message = logits[:, message_start:, :]\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits_message.reshape(-1, logits.size(-1)),\n",
        "                message.reshape(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, segment, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        self.eval()  # Set to evaluation mode\n",
        "\n",
        "        with torch.no_grad():\n",
        "            seg_len = segment.size(1)  # Store original segment length\n",
        "            for _ in range(max_new_tokens):\n",
        "                if segment.size(1) > self.config[\"block_size\"]:\n",
        "                    idx_cond = segment[:, -self.config[\"block_size\"]:]\n",
        "                else:\n",
        "                    idx_cond = segment\n",
        "\n",
        "                # Get predictions\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :] / temperature  # Take the logits for the last token\n",
        "\n",
        "                # Apply top-k sampling if specified\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "                # Sample next token\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Concatenate to segment\n",
        "                segment = torch.cat((segment, next_token), dim=1)\n",
        "\n",
        "                # Optional: Stop if end token is generated\n",
        "                if next_token.item() == self.config.get(\"eos_token\", -1):\n",
        "                    break\n",
        "\n",
        "            # Return generated sequence excluding input segment\n",
        "            return segment[:, seg_len:]\n"
      ],
      "metadata": {
        "id": "XJI3HWHhixSm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnE8bM3LowuR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "chars = sorted(set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 ,.!-()\\''))\n",
        "vocab_size = len(chars) + 1\n",
        "\n",
        "config = {\n",
        "    \"vocab_size\":vocab_size,\n",
        "    \"n_embed\": 512,\n",
        "    \"n_heads\": 8,\n",
        "    \"seg_length\": 44,\n",
        "    \"message_length\": 70,\n",
        "    \"dropout\": 0.1,\n",
        "    \"block_size\": 44 + 70,\n",
        "    \"n_layers\": 5,\n",
        "    \"batch_size\": 128,\n",
        "    \"n_epochs\": 10\n",
        "}\n",
        "\n",
        "\n",
        "segments = []\n",
        "messages = []\n",
        "\n",
        "# Extract and convert each segment and message to a tensor\n",
        "for segment_encoded, message_encoded in encoded_pairs:\n",
        "    segments.append(torch.tensor(segment_encoded))  # Convert to tensor\n",
        "    messages.append(torch.tensor(message_encoded))  # Convert to tensor\n",
        "\n",
        "# Stack the padded segments and messages into tensors (batch first)\n",
        "segments_tensor = torch.stack(segments)  # Shape: (batch_size, seg_length)\n",
        "messages_tensor = torch.stack(messages)  # Shape: (batch_size, msg_length)\n",
        "\n",
        "batch_size = 32  # Number of rows per batch\n",
        "total_samples = segments_tensor.size(0)  # Number of rows in the dataset\n",
        "\n",
        "\n",
        "def train_model(model, segments_tensor, messages_tensor, config):\n",
        "    device = next(model.parameters()).device\n",
        "    # Training setup\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"n_epochs\"])\n",
        "\n",
        "    # Calculate total batches\n",
        "    total_samples = len(segments_tensor)\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    n_batches = (total_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(config[\"n_epochs\"]):\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(0, total_samples, batch_size):\n",
        "            # Get batch\n",
        "            batch_segments = segments_tensor[i:i+batch_size]\n",
        "            batch_messages = messages_tensor[i:i+batch_size]\n",
        "\n",
        "            # Move to GPU\n",
        "            segment = batch_segments.to(device, non_blocking=True)\n",
        "            message = batch_messages.to(device, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
        "            logits, loss = model(segment, message)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            batch_idx = i // batch_size\n",
        "            if batch_idx % 10 == 0:\n",
        "                avg_loss = total_loss / (batch_idx + 1)\n",
        "                print(f\"Epoch {epoch+1}/{config['n_epochs']}, \"\n",
        "                      f\"Batch {batch_idx}/{n_batches}, \"\n",
        "                      f\"Loss: {loss.item():.4f}, \"\n",
        "                      f\"Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate epoch statistics\n",
        "        epoch_loss = total_loss / n_batches\n",
        "        print(f\"Epoch {epoch+1} completed. Average Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "HUxUGc6ztT77"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = GPT(config).to(\"cuda\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, segments_tensor, messages_tensor, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yctxy8dOo1NU",
        "outputId": "a444ee61-3940-4e5b-c5dd-5ff099fd2810"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 0/24, Loss: 4.2607, Avg Loss: 4.2607\n",
            "Epoch 1/10, Batch 10/24, Loss: 4.1493, Avg Loss: 4.1957\n",
            "Epoch 1/10, Batch 20/24, Loss: 4.0191, Avg Loss: 4.1396\n",
            "Epoch 1 completed. Average Loss: 4.1211\n",
            "Epoch 2/10, Batch 0/24, Loss: 3.9674, Avg Loss: 3.9674\n",
            "Epoch 2/10, Batch 10/24, Loss: 3.8383, Avg Loss: 3.9037\n",
            "Epoch 2/10, Batch 20/24, Loss: 3.6873, Avg Loss: 3.8337\n",
            "Epoch 2 completed. Average Loss: 3.8110\n",
            "Epoch 3/10, Batch 0/24, Loss: 3.6185, Avg Loss: 3.6185\n",
            "Epoch 3/10, Batch 10/24, Loss: 3.4404, Avg Loss: 3.5309\n",
            "Epoch 3/10, Batch 20/24, Loss: 3.2404, Avg Loss: 3.4354\n",
            "Epoch 3 completed. Average Loss: 3.4054\n",
            "Epoch 4/10, Batch 0/24, Loss: 3.1558, Avg Loss: 3.1558\n",
            "Epoch 4/10, Batch 10/24, Loss: 2.9646, Avg Loss: 3.0598\n",
            "Epoch 4/10, Batch 20/24, Loss: 2.7731, Avg Loss: 2.9631\n",
            "Epoch 4 completed. Average Loss: 2.9341\n",
            "Epoch 5/10, Batch 0/24, Loss: 2.6978, Avg Loss: 2.6978\n",
            "Epoch 5/10, Batch 10/24, Loss: 2.5450, Avg Loss: 2.6200\n",
            "Epoch 5/10, Batch 20/24, Loss: 2.3967, Avg Loss: 2.5438\n",
            "Epoch 5 completed. Average Loss: 2.5212\n",
            "Epoch 6/10, Batch 0/24, Loss: 2.3391, Avg Loss: 2.3391\n",
            "Epoch 6/10, Batch 10/24, Loss: 2.2307, Avg Loss: 2.2834\n",
            "Epoch 6/10, Batch 20/24, Loss: 2.1254, Avg Loss: 2.2290\n",
            "Epoch 6 completed. Average Loss: 2.2128\n",
            "Epoch 7/10, Batch 0/24, Loss: 2.0843, Avg Loss: 2.0843\n",
            "Epoch 7/10, Batch 10/24, Loss: 2.0142, Avg Loss: 2.0477\n",
            "Epoch 7/10, Batch 20/24, Loss: 1.9455, Avg Loss: 2.0120\n",
            "Epoch 7 completed. Average Loss: 2.0013\n",
            "Epoch 8/10, Batch 0/24, Loss: 1.9187, Avg Loss: 1.9187\n",
            "Epoch 8/10, Batch 10/24, Loss: 1.8790, Avg Loss: 1.8974\n",
            "Epoch 8/10, Batch 20/24, Loss: 1.8399, Avg Loss: 1.8766\n",
            "Epoch 8 completed. Average Loss: 1.8703\n",
            "Epoch 9/10, Batch 0/24, Loss: 1.8246, Avg Loss: 1.8246\n",
            "Epoch 9/10, Batch 10/24, Loss: 1.8068, Avg Loss: 1.8142\n",
            "Epoch 9/10, Batch 20/24, Loss: 1.7892, Avg Loss: 1.8043\n",
            "Epoch 9 completed. Average Loss: 1.8013\n",
            "Epoch 10/10, Batch 0/24, Loss: 1.7824, Avg Loss: 1.7824\n",
            "Epoch 10/10, Batch 10/24, Loss: 1.7779, Avg Loss: 1.7787\n",
            "Epoch 10/10, Batch 20/24, Loss: 1.7734, Avg Loss: 1.7754\n",
            "Epoch 10 completed. Average Loss: 1.7743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_marketing_messagez(model, segment_text, max_length=50, temperature=0.7, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate a marketing message using the trained GPT model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained GPT model\n",
        "        segment_text: String containing the customer segment description\n",
        "        max_length: Maximum length of generated message\n",
        "        temperature: Controls randomness (0.7 = balanced, <0.7 = focused, >0.7 = creative)\n",
        "        top_k: Number of top tokens to consider for sampling\n",
        "    \"\"\"\n",
        "    # Tokenize the segment\n",
        "    #model.eval()\n",
        "    segment_tokens = encode(segment_text, config['seg_length'])\n",
        "    segment_tensor = torch.tensor(segment_tokens).unsqueeze(0).to('cuda')\n",
        "\n",
        "    # Generate message tokens\n",
        "    generated_tokens = model.generate(\n",
        "        segment=segment_tensor,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k\n",
        "    )\n",
        "\n",
        "    # Decode the generated tolist())\n",
        "\n",
        "    return message\n",
        "\n",
        "segment = \"Young urban man\"\n",
        "message = generate_marketing_messagez(\n",
        "    model=model,\n",
        "    segment_text=segment,\n",
        "    max_length=64,\n",
        "    temperature=0.7,\n",
        "    top_k=50\n",
        "    )\n",
        "\n",
        "print(f\"Segment: {segment}\")\n",
        "print(f\"Generated Message: {message}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTMOEx6j2uNP",
        "outputId": "fe6bcc96-d704-46f7-b62f-3f462f1b9a1e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment: Young urban man\n",
            "Generated Message: Discover the latest technology tailored for male 51 years old!\n"
          ]
        }
      ]
    }
  ]
}